{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEFINITIVO-DQN-cartPole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "## Install Dependencies and Stable Baselines Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/hill-a/stable-baselines).\n",
        "\n",
        "```\n",
        "sudo apt-get update && sudo apt-get install cmake libopenmpi-dev zlib1g-dev\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines[mpi]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYz5-PWAFUa-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6988b4c4-982a-4164-823c-f213876002c9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M0Xfo1JRukY"
      },
      "source": [
        "#! pip install keras==2.3.1"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1983a1e9-d394-45f5-c1f8-fe0912a00816"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "#!pip install tensorflow==1.15.0\n",
        "%tensorflow_version 1.x\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
            "Requirement already satisfied: stable-baselines[mpi]==2.10.0 in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.0.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.6 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "source": [
        "import gym\n",
        "import pickle\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import time\n",
        "\n",
        "import stable_baselines\n",
        "stable_baselines.__version__\n",
        "from stable_baselines import DQN\n",
        "#from stable_baselines.common.callbacks import BaseCallback"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6YssoP-WiRa"
      },
      "source": [
        "## Resolvendo os problemas da seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hlEzo8VWhm0"
      },
      "source": [
        "seed_value = 12345\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "#seed_value += 1\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "#seed_value += 1\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "#seed_value += 1\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(seed_value)\n",
        "\n",
        "\n",
        "\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "tf.keras.backend.set_session(sess)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEm7jCLTfEcA"
      },
      "source": [
        "## Limpando pastas locais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgt4Aor9fDR2"
      },
      "source": [
        "#!rm -rf logs\n",
        "#!rm -rf results\n",
        "#!rm -rf videos"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzl0tME0Xt93"
      },
      "source": [
        "## Parâmetros e paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qREKHyLvXv5v"
      },
      "source": [
        "# AQUI\n",
        "agent = 'DQN-cartPole'\n",
        "enviroment = 'CartPole-v1'\n",
        "\n",
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "experiment_name = agent + '_' + timestr\n",
        "\n",
        "os.makedirs(experiment_name)\n",
        "\n",
        "results_path = experiment_name + '/results_' + experiment_name\n",
        "logdir = experiment_name + '/logs_' + experiment_name\n",
        "videodir = experiment_name + '/videos_' + experiment_name\n",
        "\n",
        "os.makedirs(results_path)\n",
        "os.makedirs(logdir)\n",
        "os.makedirs(videodir)\n",
        "\n",
        "\n",
        "\n",
        "param_model = {\n",
        "    'double_q': False, \n",
        "    'prioritized_replay': False,\n",
        "    'gamma': 0.99,\n",
        "    'learning_rate': 0.0005,\n",
        "    'buffer_size': 100000,\n",
        "    'exploration_fraction': 0.1, \n",
        "    'exploration_final_eps': 0.02,\n",
        "    'exploration_initial_eps': 0.1,\n",
        "    'target_network_update_freq': 500,\n",
        "    'prioritized_replay_alpha': 0.6,\n",
        "    'prioritized_replay_beta0': 0.4,\n",
        "    'prioritized_replay_beta_iters':None,\n",
        "    'prioritized_replay_eps': 1e-06,\n",
        "    'param_noise': False,\n",
        "    'learning_starts': 1000,\n",
        "    'batch_size': 32,\n",
        "    'train_freq': 1,\n",
        "    'policy_kwargs': None,\n",
        "    'verbose': 1,\n",
        "    'seed': seed_value,\n",
        "    'n_cpu_tf_sess': 1 \n",
        "    }\n",
        "\n",
        "param_learning = {\n",
        "    'total_timesteps': 10000,\n",
        "    'callback': None,\n",
        "    'log_interval': 10\n",
        "}\n",
        "\n",
        "\n",
        "log = {**param_model, **param_learning}\n",
        "\n",
        "param_model['policy_kwargs'] = dict(dueling=False)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc3GeyoEjCCD"
      },
      "source": [
        "## Implementação de callbacks e evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEXj4MrXIvr1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(model, num_episodes=100):\n",
        "    \"\"\"\n",
        "    Evaluate a RL agent\n",
        "    :param model: (BaseRLModel object) the RL Agent\n",
        "    :param num_episodes: (int) number of episodes to evaluate it\n",
        "    :return: (float) Mean reward for the last num_episodes\n",
        "    \"\"\"\n",
        "    # This function will only work for a single Environment\n",
        "    env = model.get_env()\n",
        "    all_episode_rewards = []\n",
        "    win = 0\n",
        "    loss = 0\n",
        "    for i in range(num_episodes):\n",
        "        #print(f'episode {i}')\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        lives = 0\n",
        "        obs = env.reset()\n",
        "        while not done:\n",
        "            # _states are only useful when using LSTM policies\n",
        "            action, _states = model.predict(obs, deterministic=True)\n",
        "            # here, action, rewards and dones are arrays\n",
        "            # because we are using vectorized env\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            episode_rewards.append(reward)\n",
        "             \n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "        if (all_episode_rewards[-1] >= 350):\n",
        "          win += 1\n",
        "        else:\n",
        "          loss += 1 \n",
        "\n",
        "        #print(f'reward total: {all_episode_rewards[-1]}')\n",
        "    mean_episode_reward = np.mean(all_episode_rewards)\n",
        "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
        "  \n",
        "    return all_episode_rewards, win, loss"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBHntE8Lrk5T"
      },
      "source": [
        "## Políticas customizadas aqui"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAOWPweZroek"
      },
      "source": [
        "from stable_baselines.deepq.policies import FeedForwardPolicy\n",
        "\n",
        "class simpleMLP(FeedForwardPolicy):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(simpleMLP, self).__init__(*args, **kwargs,\n",
        "                                           layers=[32, 32],\n",
        "                                           layer_norm=False,\n",
        "                                           feature_extraction=\"mlp\")\n",
        "        \n",
        "class mediumMLP(FeedForwardPolicy):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(mediumMLP, self).__init__(*args, **kwargs,\n",
        "                                           layers=[128, 64],\n",
        "                                           layer_norm=False,\n",
        "                                           feature_extraction=\"mlp\")\n",
        "\n",
        "class normMLP(FeedForwardPolicy):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(normMLP, self).__init__(*args, **kwargs,\n",
        "                                           layers=[64, 64],\n",
        "                                           layer_norm=True,\n",
        "                                           feature_extraction=\"mlp\",)  \n",
        "\n",
        "#AQUI ------ LEMBRAR DE COLOCAR O NOME DA FUNCAO OU DA POLITICA JA DEFINIDA PELA BIBLIOTECA \n",
        "policy_name = 'LnMlpPolicy'   #ESSA VARIAVEL TEM O NOME DA POLITICA QUE SERA USADA\n",
        "log['policy_name'] = policy_name\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prDQY1pkrkOA"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAxAkx87DRi9"
      },
      "source": [
        "## Model and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR6MsNoZYLqz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "4326f993-253e-4f41-962a-dc08b77630eb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "##ESSA\n",
        "from stable_baselines.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "import time\n",
        "\n",
        "env = gym.make(enviroment)\n",
        "env.seed(seed_value)\n",
        "\n",
        "eval_env = gym.make(enviroment)\n",
        "env.seed(seed_value)\n",
        "\n",
        "\n",
        "\n",
        "best_model_path = os.path.join(results_path, 'best_model')\n",
        "#####ESSA\n",
        "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=500, verbose=1)\n",
        "####ESSA\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path=best_model_path,\n",
        "                             callback_on_new_best=callback_on_best,\n",
        "                             log_path=logdir, eval_freq=100,\n",
        "                             deterministic=True, render=False)\n",
        "\n",
        "param_learning['callback'] = eval_callback\n",
        "\n",
        "#PASSA O POLICY NAME OU O NOME DA FUNCAO: POLICY NAME APENAS SE JÁ FOR PADRAO \n",
        "dqn_model = DQN(policy_name, env, **param_model, tensorboard_log=logdir)\n",
        "#dqn_model = DQN(simpleMLP, env, **param_model, tensorboard_log=logdir)\n",
        "dqn_model.set_random_seed(seed=seed_value)\n",
        "\n",
        "# Random Agent, before training\n",
        "all_episode_reward_before_train, win, loss = evaluate(dqn_model, num_episodes=100)\n",
        "\n",
        "log['all_reward_before_train'] = all_episode_reward_before_train\n",
        "\n",
        "plt.plot(all_episode_reward_before_train)\n",
        "plt.ylabel('Reward')\n",
        "plt.xlabel('Game')\n",
        "plt.title('Episode x Reward')\n",
        "plt.ylim(0, 600)\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean reward: 11.57 Num episodes: 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUWGZp3i9wyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9af590-7ba9-4ba3-8819-5e47b025e8c3"
      },
      "source": [
        "\n",
        "#### Treina\n",
        "start_time = time.time()\n",
        "dqn_model.learn(**param_learning)\n",
        "log['time'] = (time.time() - start_time) \n",
        "print('Fininsh --- %s seconds --- ' % log['time'])\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/stable_baselines/common/callbacks.py:277: UserWarning: Training and eval env are not of the same type<TimeLimit<CartPoleEnv<CartPole-v1>>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fa945f62860>\n",
            "  \"{} != {}\".format(self.training_env, self.eval_env))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval num_timesteps=100, episode_reward=12.80 +/- 5.64\n",
            "Episode length: 12.80 +/- 5.64\n",
            "New best mean reward!\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 9        |\n",
            "| episodes                | 10       |\n",
            "| mean 100 episode reward | 13.3     |\n",
            "| steps                   | 120      |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=200, episode_reward=12.00 +/- 3.29\n",
            "Episode length: 12.00 +/- 3.29\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 7        |\n",
            "| episodes                | 20       |\n",
            "| mean 100 episode reward | 13.5     |\n",
            "| steps                   | 257      |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=300, episode_reward=11.80 +/- 2.23\n",
            "Episode length: 11.80 +/- 2.23\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 6        |\n",
            "| episodes                | 30       |\n",
            "| mean 100 episode reward | 13.6     |\n",
            "| steps                   | 395      |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=400, episode_reward=14.40 +/- 7.84\n",
            "Episode length: 14.40 +/- 7.84\n",
            "New best mean reward!\n",
            "Eval num_timesteps=500, episode_reward=13.00 +/- 3.79\n",
            "Episode length: 13.00 +/- 3.79\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 5        |\n",
            "| episodes                | 40       |\n",
            "| mean 100 episode reward | 13.4     |\n",
            "| steps                   | 522      |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=600, episode_reward=13.20 +/- 3.49\n",
            "Episode length: 13.20 +/- 3.49\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 4        |\n",
            "| episodes                | 50       |\n",
            "| mean 100 episode reward | 13.1     |\n",
            "| steps                   | 640      |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=700, episode_reward=10.40 +/- 0.80\n",
            "Episode length: 10.40 +/- 0.80\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 3        |\n",
            "| episodes                | 60       |\n",
            "| mean 100 episode reward | 12.8     |\n",
            "| steps                   | 755      |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=800, episode_reward=9.40 +/- 1.02\n",
            "Episode length: 9.40 +/- 1.02\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 70       |\n",
            "| mean 100 episode reward | 12.9     |\n",
            "| steps                   | 893      |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=900, episode_reward=11.60 +/- 2.06\n",
            "Episode length: 11.60 +/- 2.06\n",
            "Eval num_timesteps=1000, episode_reward=12.20 +/- 1.94\n",
            "Episode length: 12.20 +/- 1.94\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 80       |\n",
            "| mean 100 episode reward | 12.8     |\n",
            "| steps                   | 1014     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=1100, episode_reward=9.40 +/- 0.80\n",
            "Episode length: 9.40 +/- 0.80\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 90       |\n",
            "| mean 100 episode reward | 12.5     |\n",
            "| steps                   | 1111     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=1200, episode_reward=9.20 +/- 0.40\n",
            "Episode length: 9.20 +/- 0.40\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 12.2     |\n",
            "| steps                   | 1203     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 110      |\n",
            "| mean 100 episode reward | 11.8     |\n",
            "| steps                   | 1297     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=1300, episode_reward=9.00 +/- 0.63\n",
            "Episode length: 9.00 +/- 0.63\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 120      |\n",
            "| mean 100 episode reward | 11.3     |\n",
            "| steps                   | 1392     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=1400, episode_reward=12.80 +/- 1.94\n",
            "Episode length: 12.80 +/- 1.94\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 130      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 1490     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=1500, episode_reward=9.60 +/- 0.49\n",
            "Episode length: 9.60 +/- 0.49\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 140      |\n",
            "| mean 100 episode reward | 10.7     |\n",
            "| steps                   | 1587     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=1600, episode_reward=9.60 +/- 0.49\n",
            "Episode length: 9.60 +/- 0.49\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 150      |\n",
            "| mean 100 episode reward | 10.4     |\n",
            "| steps                   | 1682     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=1700, episode_reward=9.20 +/- 0.75\n",
            "Episode length: 9.20 +/- 0.75\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 160      |\n",
            "| mean 100 episode reward | 10.3     |\n",
            "| steps                   | 1781     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=1800, episode_reward=9.20 +/- 0.40\n",
            "Episode length: 9.20 +/- 0.40\n",
            "Eval num_timesteps=1900, episode_reward=17.60 +/- 1.96\n",
            "Episode length: 17.60 +/- 1.96\n",
            "New best mean reward!\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 170      |\n",
            "| mean 100 episode reward | 10.3     |\n",
            "| steps                   | 1927     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2000, episode_reward=9.80 +/- 0.40\n",
            "Episode length: 9.80 +/- 0.40\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 180      |\n",
            "| mean 100 episode reward | 10.4     |\n",
            "| steps                   | 2050     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2100, episode_reward=10.20 +/- 0.98\n",
            "Episode length: 10.20 +/- 0.98\n",
            "Eval num_timesteps=2200, episode_reward=11.00 +/- 1.10\n",
            "Episode length: 11.00 +/- 1.10\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 190      |\n",
            "| mean 100 episode reward | 11.1     |\n",
            "| steps                   | 2220     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2300, episode_reward=53.00 +/- 25.81\n",
            "Episode length: 53.00 +/- 25.81\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2400, episode_reward=66.60 +/- 5.75\n",
            "Episode length: 66.60 +/- 5.75\n",
            "New best mean reward!\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 12.3     |\n",
            "| steps                   | 2430     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=2500, episode_reward=44.00 +/- 38.36\n",
            "Episode length: 44.00 +/- 38.36\n",
            "Eval num_timesteps=2600, episode_reward=90.60 +/- 25.50\n",
            "Episode length: 90.60 +/- 25.50\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2700, episode_reward=76.40 +/- 17.12\n",
            "Episode length: 76.40 +/- 17.12\n",
            "Eval num_timesteps=2800, episode_reward=77.40 +/- 10.21\n",
            "Episode length: 77.40 +/- 10.21\n",
            "Eval num_timesteps=2900, episode_reward=30.40 +/- 10.31\n",
            "Episode length: 30.40 +/- 10.31\n",
            "Eval num_timesteps=3000, episode_reward=88.60 +/- 36.43\n",
            "Episode length: 88.60 +/- 36.43\n",
            "Eval num_timesteps=3100, episode_reward=211.40 +/- 49.60\n",
            "Episode length: 211.40 +/- 49.60\n",
            "New best mean reward!\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 210      |\n",
            "| mean 100 episode reward | 18.5     |\n",
            "| steps                   | 3148     |\n",
            "--------------------------------------\n",
            "Eval num_timesteps=3200, episode_reward=254.80 +/- 79.96\n",
            "Episode length: 254.80 +/- 79.96\n",
            "New best mean reward!\n",
            "Eval num_timesteps=3300, episode_reward=309.40 +/- 78.11\n",
            "Episode length: 309.40 +/- 78.11\n",
            "New best mean reward!\n",
            "Eval num_timesteps=3400, episode_reward=268.20 +/- 197.12\n",
            "Episode length: 268.20 +/- 197.12\n",
            "Eval num_timesteps=3500, episode_reward=167.00 +/- 107.89\n",
            "Episode length: 167.00 +/- 107.89\n",
            "Eval num_timesteps=3600, episode_reward=305.20 +/- 123.24\n",
            "Episode length: 305.20 +/- 123.24\n",
            "Eval num_timesteps=3700, episode_reward=221.40 +/- 29.57\n",
            "Episode length: 221.40 +/- 29.57\n",
            "Eval num_timesteps=3800, episode_reward=289.00 +/- 81.72\n",
            "Episode length: 289.00 +/- 81.72\n",
            "Eval num_timesteps=3900, episode_reward=403.00 +/- 91.33\n",
            "Episode length: 403.00 +/- 91.33\n",
            "New best mean reward!\n",
            "Eval num_timesteps=4000, episode_reward=186.20 +/- 36.33\n",
            "Episode length: 186.20 +/- 36.33\n",
            "Eval num_timesteps=4100, episode_reward=402.00 +/- 44.44\n",
            "Episode length: 402.00 +/- 44.44\n",
            "Eval num_timesteps=4200, episode_reward=387.40 +/- 101.27\n",
            "Episode length: 387.40 +/- 101.27\n",
            "Eval num_timesteps=4300, episode_reward=298.40 +/- 127.61\n",
            "Episode length: 298.40 +/- 127.61\n",
            "Eval num_timesteps=4400, episode_reward=360.00 +/- 91.69\n",
            "Episode length: 360.00 +/- 91.69\n",
            "Eval num_timesteps=4500, episode_reward=425.20 +/- 113.52\n",
            "Episode length: 425.20 +/- 113.52\n",
            "New best mean reward!\n",
            "Eval num_timesteps=4600, episode_reward=114.80 +/- 21.75\n",
            "Episode length: 114.80 +/- 21.75\n",
            "Eval num_timesteps=4700, episode_reward=235.60 +/- 46.40\n",
            "Episode length: 235.60 +/- 46.40\n",
            "Eval num_timesteps=4800, episode_reward=251.20 +/- 17.27\n",
            "Episode length: 251.20 +/- 17.27\n",
            "Eval num_timesteps=4900, episode_reward=500.00 +/- 0.00\n",
            "Episode length: 500.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Stopping training because the mean reward 500.00  is above the threshold 500\n",
            "Fininsh --- 39.21549463272095 seconds --- \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic07IU4bLAjo"
      },
      "source": [
        "## Avaliando o agente treinado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwMs0n9PLo1q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "31be5b14-bf55-4b18-b1a6-63eaf7f74f46"
      },
      "source": [
        "log['rewards'], log['win'], log['loss']  = evaluate(dqn_model, num_episodes=100)\n",
        "log['mean'] = np.mean(log['rewards'])\n",
        "log['std'] = np.std(log['rewards'])\n",
        "\n",
        "plt.plot(log['rewards'])\n",
        "plt.ylabel('Reward')\n",
        "plt.xlabel('Game')\n",
        "plt.title('Episode x Reward')\n",
        "plt.ylim(0, 600)\n",
        "plt.show()\n",
        "\n",
        "print(\"Mean reward:\", log['mean'], \"Std reward:\", log['std'])\n",
        "print(\"win:\", log['win'], \"loss:\", log['loss'])\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean reward: 500.0 Num episodes: 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Mean reward: 500.0 Std reward: 0.0\n",
            "win: 100 loss: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-PDcR_hC3LG"
      },
      "source": [
        "## Funções auxiliares para video\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ss2zmw2C-qA"
      },
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', name=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(name)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "source": [
        "from stable_baselines.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=1000, prefix='', video_folder= videodir):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(enviroment)])\n",
        "  # Start the video at step=0 and record X steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Visualizando o agente treinado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X00RNGyoV28r"
      },
      "source": [
        "def save_log(log, agent, path, experiment_name):\n",
        "  #path = os.path.abspath(os.getcwd())\n",
        "  #path = os.path.join(path, 'results')\n",
        "  print('Save log in %s ' % path)\n",
        "  #timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "  name_file = experiment_name + '.pkl'\n",
        "  name_file = os.path.join(path, name_file)\n",
        "  if not os.path.exists(path):\n",
        "      os.makedirs(path)\n",
        "  a_file = open(name_file, \"wb\")\n",
        "  pickle.dump(log, a_file)\n",
        "  a_file.close()\n",
        "\n",
        "  name_file = experiment_name\n",
        "  #salvando o video\n",
        "  record_video(enviroment, dqn_model, video_length=6000, prefix=name_file, video_folder=videodir)\n",
        "  print('video - Saving local')\n",
        "  #mostrando o video salvo\n",
        "  show_videos(videodir, name=name_file)\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcbR6W6F-7T6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810c833a-cd20-4ad0-c109-f82c79986416"
      },
      "source": [
        "save_log(log, agent, results_path, experiment_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save log in DQN-cartPole_20210125-190415/results_DQN-cartPole_20210125-190415 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGBgBLIqRZEB"
      },
      "source": [
        "## TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTH3Sh9CRbab"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIf0AMRoUggQ"
      },
      "source": [
        "%tensorboard --logdir \"$experiment_name\"/logs_\"$experiment_name\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIQpuYVTRxK9"
      },
      "source": [
        "!cp -r \"$experiment_name\" /content/drive/MyDrive/T2_ReinforcementLearning/Results/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-RRihPqRthN"
      },
      "source": [
        "#!tensorboard dev upload \\\n",
        "#  --logdir logs \\\n",
        "#  --name \"(optional) My latest experiment\" \\\n",
        "#  --description \"(optional) Simple comparison of several hyperparameters\" \\\n",
        "#  --one_shot"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}