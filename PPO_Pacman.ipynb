{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "original-PPO-Pacman.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "## Install Dependencies and Stable Baselines Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/hill-a/stable-baselines).\n",
        "\n",
        "```\n",
        "sudo apt-get update && sudo apt-get install cmake libopenmpi-dev zlib1g-dev\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines[mpi]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYz5-PWAFUa-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "467584ed-7f3c-4f28-a5e2-6655249e7f7e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWskDE2c9WoN",
        "outputId": "440ae95d-783d-4dc9-dfbc-2a94fdbe3ad7"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "#!pip install tensorflow==1.13.2\n",
        "#%tensorflow_version 1.x\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "#!pip install stable-baselines[mpi]==2.10.0\n",
        "!pip install stable-baselines3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (1.1.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (1.7.0+cu101)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines3) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines3) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines3) (2018.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3) (0.16.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable-baselines3) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable-baselines3) (1.4.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->stable-baselines3) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "source": [
        "import gym\n",
        "import pickle\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import time\n",
        "\n",
        "import stable_baselines3\n",
        "#stable_baselines.__version__\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "#from stable_baselines.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "#from stable_baselines.common.vec_env import unwrap_vec_normalize\n",
        "#from stable_baselines.common.policies import ActorCriticCnnPolicy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEm7jCLTfEcA"
      },
      "source": [
        "## Limpando pastas locais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgt4Aor9fDR2"
      },
      "source": [
        "#!rm -rf logs\n",
        "#!rm -rf results\n",
        "#!rm -rf videos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzl0tME0Xt93"
      },
      "source": [
        "## Parâmetros e paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qREKHyLvXv5v"
      },
      "source": [
        "# AQUI\n",
        "agent = 'PPO'\n",
        "enviroment = 'MsPacman-v0'\n",
        "\n",
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "experiment_name = agent + '_' + timestr\n",
        "\n",
        "os.makedirs(experiment_name)\n",
        "\n",
        "results_path = experiment_name + '/results_' + experiment_name\n",
        "logdir = experiment_name + '/logs_' + experiment_name\n",
        "videodir = experiment_name + '/videos_' + experiment_name\n",
        "\n",
        "param_model = {\n",
        "    'learning_rate': 0.0003,\n",
        "    'n_steps': 100,\n",
        "    'batch_size': 12,\n",
        "    'n_epochs': 10,\n",
        "    'gamma': 0.99,\n",
        "    'gae_lambda': 0.95,\n",
        "    'clip_range': 0.2,\n",
        "    'clip_range_vf': None,\n",
        "    'ent_coef': 0.0,\n",
        "    'vf_coef': 0.5,\n",
        "    'max_grad_norm': 0.5,\n",
        "    'use_sde': False,\n",
        "    'sde_sample_freq': -1,\n",
        "    'target_kl': None,\n",
        "    'policy_kwargs': None,\n",
        "    'verbose': 0,\n",
        "    'seed': 1278,\n",
        "    'device': 'cuda'\n",
        "}\n",
        "\n",
        "param_learning = {\n",
        "    'total_timesteps': 10000,\n",
        "    'callback': None,\n",
        "    'log_interval': 1,\n",
        "    'reset_num_timesteps' : True\n",
        "}\n",
        "\n",
        "dim_features = 64\n",
        "policy_name = 'CnnPolicy'\n",
        "\n",
        "log = {**param_model, **param_learning}\n",
        "\n",
        "log['policy_name'] = policy_name\n",
        "log['dim_features'] = dim_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc3GeyoEjCCD"
      },
      "source": [
        "## Implementação de callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay3dHqzrjBd0"
      },
      "source": [
        "import numpy as np\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "class EvalCallback(BaseCallback):\n",
        "  \"\"\"\n",
        "  Callback for evaluating an agent.\n",
        "\n",
        "  :param eval_env: (gym.Env) The environment used for initialization\n",
        "  :param n_eval_episodes: (int) The number of episodes to test the agent\n",
        "  :param eval_freq: (int) Evaluate the agent every eval_freq call of the callback.\n",
        "  \"\"\"\n",
        "  def __init__(self, eval_env, deterministic=True, n_eval_episodes=5, eval_freq=200, path=results_path, verbose=1):\n",
        "    super(EvalCallback, self).__init__(verbose)\n",
        "    self.eval_env = eval_env\n",
        "    self.deterministic = deterministic\n",
        "    self.n_eval_episodes = n_eval_episodes\n",
        "    self.eval_freq = eval_freq\n",
        "    self.best_mean_reward = -np.inf\n",
        "    self.save_path = os.path.join(path, 'best_model')\n",
        "\n",
        "  def _on_step(self) -> bool:\n",
        "    \"\"\"\n",
        "    This method will be called by the model.\n",
        "\n",
        "    :return: (bool)\n",
        "    \"\"\"\n",
        "    # self.n_calls is automatically updated because\n",
        "    # we derive from BaseCallback\n",
        "    if self.n_calls % self.eval_freq == 0:\n",
        "      # Evaluate the agent:\n",
        "      done = False\n",
        "      all_episode_rewards = []\n",
        "      lives = 0\n",
        "      win = 0\n",
        "      loss = 0\n",
        "      obs = self.eval_env.reset()\n",
        "      for _ in range(self.n_eval_episodes):\n",
        "        episode_rewards = []\n",
        "        while not done:\n",
        "          action, _states = self.model.predict(obs, deterministic=self.deterministic)\n",
        "          obs, reward, done, info = self.eval_env.step(action)\n",
        "          lives = info[-1]['ale.lives']\n",
        "          episode_rewards.append(reward)\n",
        "        if (lives > 0):\n",
        "          win += 1\n",
        "        else:\n",
        "          loss += 1  \n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "      # Save the agent if needed\n",
        "      # and update self.best_mean_reward\n",
        "      mean_reward = np.mean(all_episode_rewards)\n",
        "\n",
        "      print(f'timestep {self.n_calls} - Mean reward evaluation: {self.best_mean_reward} - win: {win} loss: {loss}')\n",
        "\n",
        "      if mean_reward > self.best_mean_reward:\n",
        "        self.best_mean_reward = mean_reward\n",
        "        self.model.save(self.save_path)\n",
        "        if self.verbose > 0:\n",
        "          print(f'BEST MEAN REWARD FOUND: {self.best_mean_reward}')\n",
        "          print(f'Saving new best model to {self.save_path}.zip')\n",
        "          print('-' * 30)        \n",
        "      # ====================== #    \n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAxAkx87DRi9"
      },
      "source": [
        "## Model and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "source": [
        "\n",
        "#### AQUI\n",
        "log['feature_extractor_arch'] = {'name': 'CNN', 'input_ch': [3, 32], \n",
        "                                 'output_ch': [32, 64], 'kernel_size': [8, 4], \n",
        "                                 'stride': [4, 2], 'padding': [0, 0], \n",
        "                                 'sequence': ['cnn2d', 'relu', 'cnn2d', 'relu', \n",
        "                                              'flatten', 'linear', 'relu']}  \n",
        "\n",
        "class CustomCNN(BaseFeaturesExtractor):\n",
        "  \"\"\"\n",
        "  :param observation_space: (gym.Space)\n",
        "  :param features_dim: (int) Number of features extracted.\n",
        "      This corresponds to the number of unit for the last layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
        "      super(CustomCNN, self).__init__(observation_space, features_dim)\n",
        "      # We assume CxHxW images (channels first)\n",
        "      # Re-ordering will be done by pre-preprocessing or wrapper\n",
        "      n_input_channels = observation_space.shape[0]\n",
        "      self.cnn = nn.Sequential(\n",
        "          nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Flatten(),\n",
        "      )\n",
        "\n",
        "      # Compute shape by doing one forward pass\n",
        "      with th.no_grad():\n",
        "          n_flatten = self.cnn(\n",
        "              th.as_tensor(observation_space.sample()[None]).float()\n",
        "          ).shape[1]\n",
        "\n",
        "      self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "  def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "      return self.linear(self.cnn(observations))\n",
        "\n",
        "# AQUI\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=CustomCNN,\n",
        "    features_extractor_kwargs=dict(features_dim=dim_features),\n",
        "    #net_arch=[128, 164],\n",
        "    #activation_fn=th.nn.Tanh \n",
        ")\n",
        "\n",
        "param_model['policy_kwargs'] = policy_kwargs\n",
        "\n",
        "if 'net_arch' in policy_kwargs:\n",
        "  log['net_arch'] = policy_kwargs['net_arch']\n",
        "\n",
        "if 'activation_fn' in policy_kwargs:\n",
        "  log['activation_fn'] = policy_kwargs['activation_fn']  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR6MsNoZYLqz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "eef20e2f-a099-4107-af67-789984cf5a51"
      },
      "source": [
        "#env = make_vec_env(enviroment, n_envs=4)\n",
        "#env.seed(10294)\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "import time\n",
        "\n",
        "env = gym.make(enviroment)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "\n",
        "eval_env = gym.make(enviroment)\n",
        "eval_env = DummyVecEnv([lambda: eval_env])\n",
        "\n",
        "callback = EvalCallback(eval_env, deterministic=True, n_eval_episodes=30, \n",
        "                        eval_freq=100, path=results_path, verbose=1)\n",
        "\n",
        "param_learning['callback'] = callback\n",
        "\n",
        "model = PPO(policy_name, env, **param_model, tensorboard_log=logdir)\n",
        "model.set_random_seed(seed=12345)\n",
        "\n",
        "start_time = time.time()\n",
        "model.learn(**param_learning)\n",
        "log['time'] = (time.time() - start_time) \n",
        "print('Fininsh --- %s seconds --- ' % log['time'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-a95518ae5942>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparam_learning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fininsh --- %s seconds --- '\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;31m# Give access to local variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_locals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36mon_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-090c8be1c48b>\u001b[0m in \u001b[0;36m_on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m           loss += 1 '''\n\u001b[1;32m     48\u001b[0m       \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m       \u001b[0mmean_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;31m#all_episode_rewards.append(sum(episode_rewards))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;31m# Save the agent if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'DummyVecEnv' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic07IU4bLAjo"
      },
      "source": [
        "## Avaliando o agente treinado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEXj4MrXIvr1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(model, det, num_episodes=100):\n",
        "    \"\"\"\n",
        "    Evaluate a RL agent\n",
        "    :param model: (BaseRLModel object) the RL Agent\n",
        "    :param num_episodes: (int) number of episodes to evaluate it\n",
        "    :return: (float) Mean reward for the last num_episodes\n",
        "    \"\"\"\n",
        "    # This function will only work for a single Environment\n",
        "    env = model.get_env()\n",
        "    all_episode_rewards = []\n",
        "    win = 0\n",
        "    loss = 0\n",
        "    for i in range(num_episodes):\n",
        "        print(f'episode {i}')\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        lives = 0\n",
        "        obs = env.reset()\n",
        "        while not done:\n",
        "            # _states are only useful when using LSTM policies\n",
        "            action, _states = model.predict(obs, deterministic = det)\n",
        "            # here, action, rewards and dones are arrays\n",
        "            # because we are using vectorized env\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            lives = info[-1]['ale.lives']\n",
        "            episode_rewards.append(reward)\n",
        "        if (lives > 0):\n",
        "          win += 1\n",
        "        else:\n",
        "          loss += 1  \n",
        "\n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "        print(f'reward total: {all_episode_rewards[-1]}')\n",
        "    mean_episode_reward = np.mean(all_episode_rewards)\n",
        "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
        "  \n",
        "    return all_episode_rewards, win, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwMs0n9PLo1q"
      },
      "source": [
        "log['rewards'], log['win'], log['loss']  = evaluate(model, det=True, num_episodes=30)\n",
        "log['mean'] = np.mean(log['rewards'])\n",
        "log['std'] = np.std(log['rewards'])\n",
        "\n",
        "plt.plot(log['rewards'])\n",
        "plt.ylabel('Reward')\n",
        "plt.xlabel('Game')\n",
        "plt.title('Game x Reward')\n",
        "plt.show()\n",
        "\n",
        "print(\"Mean reward:\", log['mean'], \"Std reward:\", log['std'])\n",
        "print(\"win:\", log['win'], \"loss:\", log['loss'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-PDcR_hC3LG"
      },
      "source": [
        "## Funções auxiliares para video\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ss2zmw2C-qA"
      },
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', name=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(name)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=1000, prefix='', video_folder= videodir):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(enviroment)])\n",
        "  # Start the video at step=0 and record X steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Visualizando o agente treinado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X00RNGyoV28r"
      },
      "source": [
        "def save_log(log, agent, path):\n",
        "  #path = os.path.abspath(os.getcwd())\n",
        "  #path = os.path.join(path, 'results')\n",
        "  print('Save log in %s ' % path)\n",
        "  #timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "  name_file = agent + '-' + timestr + '.pkl'\n",
        "  name_file = os.path.join(path, name_file)\n",
        "  if not os.path.exists(path):\n",
        "      os.makedirs(path)\n",
        "  a_file = open(name_file, \"wb\")\n",
        "  pickle.dump(log, a_file)\n",
        "  a_file.close()\n",
        "\n",
        "  name_file = agent + '-' + timestr\n",
        "  #salvando o video\n",
        "  record_video(enviroment, model, video_length=1000, prefix=name_file, video_folder=videodir)\n",
        "  print('video - Saving local')\n",
        "  #mostrando o video salvo\n",
        "  show_videos(videodir, name=name_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcbR6W6F-7T6"
      },
      "source": [
        "save_log(log, agent, results_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGBgBLIqRZEB"
      },
      "source": [
        "## TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTH3Sh9CRbab"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIf0AMRoUggQ"
      },
      "source": [
        "%tensorboard --logdir \"$experiment_name\"/logs_\"$experiment_name\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIQpuYVTRxK9"
      },
      "source": [
        "!cp -r \"$experiment_name\" /content/drive/MyDrive/T2_ReinforcementLearning/Results/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-RRihPqRthN"
      },
      "source": [
        "#!tensorboard dev upload \\\n",
        "#  --logdir logs \\\n",
        "#  --name \"(optional) My latest experiment\" \\\n",
        "#  --description \"(optional) Simple comparison of several hyperparameters\" \\\n",
        "#  --one_shot"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}